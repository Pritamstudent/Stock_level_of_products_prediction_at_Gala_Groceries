# -*- coding: utf-8 -*-
"""eda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wg_XCfaped60iUseils0W77xXOeVkBY6

# Task 1 - Exploratory Data Analysis

This notebook will walk you through this task interactively, meaning that once you've imported this notebook into `Google Colab`, you'll be able to run individual cells of code independantly, and see the results as you go.

This notebooks is designed for users that have an understanding of Python and data analysis. There will be some helper functions and initial setup code provided, but it will be up to you to perform the analysis and to draw insights!

---

## Section 1 - Setup

First, we need to mount this notebook to our Google Drive folder, in order to access the CSV data file. If you haven't already, watch this video https://www.youtube.com/watch?v=woHxvbBLarQ to help you mount your Google Drive folder.
"""

from google.colab import drive
drive.mount('/content/drive')

"""In order to view, analyse and manipulate the dataset, we must load it into something called a `dataframe`, which is a way of storing tabulated data in a virtual table. This dataframe will allow us to analyse the data freely. To load it into a dataframe, we will need a package called `Pandas`. We can install pandas with this command:"""

!pip install pandas

"""And now we can import this package like so:"""

import pandas as pd

"""---

## Section 2 - Data loading

Now that Google Drive is mounted, you can store the CSV file anywhere in your Drive and update the `path` variable below to access it within this notebook. Once we've updated the `path`, let's read this CSV file into a pandas dataframe and see what it looks like
"""

path = "/content/drive/MyDrive/sample_sales_data.csv"
df = pd.read_csv(path)
df.drop(columns=["Unnamed: 0"], inplace=True, errors='ignore')
df.head()

"""Using the `.head()` method allows us to see the top 5 (5 by default) rows within the dataframe. We can use `.tail()` to see the bottom 5. If you want to see more than 5 rows, simply enter a number into the parentheses, e.g. `head(10)` or `tail(10)`.

---

## Section 3 - Descriptive statistics

In this section, you should try to gain a description of the data, that is: what columns are present, how many null values exist and what data types exists within each column.

To get you started an explanation of what the column names mean are provided below:

- transaction_id = this is a unique ID that is assigned to each transaction
- timestamp = this is the datetime at which the transaction was made
- product_id = this is an ID that is assigned to the product that was sold. Each product has a unique ID
- category = this is the category that the product is contained within
- customer_type = this is the type of customer that made the transaction
- unit_price = the price that 1 unit of this item sells for
- quantity = the number of units sold for this product within this transaction
- total = the total amount payable by the customer
- payment_type = the payment method used by the customer

After this, you should try to compute some descriptive statistics of the numerical columns within the dataset, such as:

- mean
- median
- count
- etc...
"""

df.info()

df['product_id'].value_counts()

df['category'].value_counts()

df['customer_type'].value_counts()

df['payment_type'].value_counts()

df[df.isna().any(axis=1)]

"""None of the rows have any null values , so it does not have any missing values. We will use the describe function to find the mean,median count, etc."""

df.describe()

"""---

## Section 4 - Visualisation

Now that you've computed some descriptive statistics of the dataset, let's create some visualisations. You may use any package that you wish for visualisation, however, some helper functions have been provided that make use of the `seaborn` package. If you wish to use these helper functions, ensure to run the below cells that install and import `seaborn`.
"""

!pip install seaborn

import seaborn as sns

"""To analyse the dataset, below are snippets of code that you can use as helper functions to visualise different columns within the dataset. They include:

- plot_continuous_distribution = this is to visualise the distribution of numeric columns
- get_unique_values = this is to show how many unique values are present within a column
- plot_categorical_distribution = this is to visualise the distribution of categorical columns
- correlation_plot = this is to plot the correlations between the numeric columns within the data
"""

def plot_continuous_distribution(data: pd.DataFrame = None, column: str = None, height: int = 8):
  _ = sns.displot(data, x=column, kde=True, height=height, aspect=height/5).set(title=f'Distribution of {column}');

def get_unique_values(data, column):
  num_unique_values = len(data[column].unique())
  value_counts = data[column].value_counts()
  print(f"Column: {column} has {num_unique_values} unique values\n")
  print(value_counts)

def plot_categorical_distribution(data: pd.DataFrame = None, column: str = None, height: int = 8, aspect: int = 2):
  _ = sns.catplot(data=data, x=column, kind='count', height=height, aspect=aspect).set(title=f'Distribution of {column}');

def correlation_plot(data: pd.DataFrame = None):
  corr = df.corr()
  corr.style.background_gradient(cmap='coolwarm')

plot_continuous_distribution(df , "quantity")

"""Here, in the distribution of the quantity, we only have 4 unique values and all of them are evenly ditributed. This means that the customers are evenly buring quantities from 1 to 4."""

plot_continuous_distribution(df , "unit_price")

"""Here, in the distribution of the unit price, we can see that the distribution of ethe unit prices is not even. ALso, most of the product are at a cheap rate rather than being expensive."""

plot_continuous_distribution(df , "total")

"""Now, we know total = unit price x quantity. Since unit price distribution is positively skewed, so the total distribution will also be skewed. In general, we also know that the grocery store will sell the cheap products more rather than expensive products.

**Categorical Data**
"""

get_unique_values(df,'transaction_id')

"""From the above information, we can see that we will have each unique transaction ID for each of the payment."""

get_unique_values(df,'product_id')

"""Here, it is the ID of the products and how frequently it has been sold. The top ID has been sold 114 times and lowest one only 3 times."""

get_unique_values(df,'payment_type')

"""We have 4 unique values in the payment_type column and cash has been used most frequently and debit card has been used least frequently."""

get_unique_values(df,'customer_type')

"""The customer type has 5 unique values and most of the customers are non-members of the grocery shop."""

get_unique_values(df,'category')

"""category has 22 unique values and most of the items sold belong to the fruit category and the least item sold belong to the spices and herbs."""

plot_categorical_distribution(df,'category')

plot_categorical_distribution(df,'payment_type')

plot_categorical_distribution(df,'customer_type')

"""Now, let us see the time stamp column"""

df['timestamp'].dtype

"""Now, let us change that into datetime type."""

def convert(data: pd.DataFrame = None, col: str = None):
  dummy = data.copy()
  dummy[col] = pd.to_datetime(dummy[col], format= '%Y-%m-%d %H:%M:%S')
  return dummy

df = convert(df, 'timestamp')
df.info()

"""Let us do something interesting.  Let us see at which hour we have the most number of transactions.

"""

df_new = df.copy()
df_new['hour'] = df['timestamp'].dt.hour
df_new.head()

get_unique_values(df_new,'hour')

"""We see most of the transactions are made before lunch, and after the job time.

Now it is your chance to visualise the columns, give it your best shot! As well as simply visualising the columns, try to interpret what the results mean in the context of the client.
"""

corr = df_new.corr()
corr.style.background_gradient(cmap='coolwarm')

"""---

## Section 5 - Summary

We have completed an initial exploratory data analysis on the sample of data provided. We should now have a solid understanding of the data.

The client wants to know

```
"How to better stock the items that they sell"
```

From this dataset, it is impossible to answer that question. In order to make the next step on this project with the client, it is clear that:

- We need more rows of data. The current sample is only from 1 store and 1 week worth of data
- We need to frame the specific problem statement that we want to solve. The current business problem is too broad, we should narrow down the focus in order to deliver a valuable end product
- We need more features. Based on the problem statement that we move forward with, we need more columns (features) that may help us to understand the outcome that we're solving for


"""