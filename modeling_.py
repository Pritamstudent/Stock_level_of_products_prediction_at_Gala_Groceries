# -*- coding: utf-8 -*-
"""modeling .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LtkobhumG7qG6xQcP7KmTKS3NobeUUBp

# Task 3 - Modeling

This notebook will get you started by helping you to load the data, but then it'll be up to you to complete the task! If you need help, refer to the `modeling_walkthrough.ipynb` notebook.


## Section 1 - Setup

First, we need to mount this notebook to our Google Drive folder, in order to access the CSV data file. If you haven't already, watch this video https://www.youtube.com/watch?v=woHxvbBLarQ to help you mount your Google Drive folder.
"""

from google.colab import drive
drive.mount('/content/drive')

"""We want to use dataframes once again to store and manipulate the data."""

!pip install pandas

import pandas as pd

"""---

## Section 2 - Data loading

Similar to before, let's load our data from Google Drive for the 3 datasets provided. Be sure to upload the datasets into Google Drive, so that you can access them here.
"""

path = "/content/drive/MyDrive/"

sales_df = pd.read_csv(f"{path}sales.csv")
sales_df.drop(columns=["Unnamed: 0"], inplace=True, errors='ignore')
sales_df.head()

stock_df = pd.read_csv(f"{path}sensor_stock_levels.csv")
stock_df.drop(columns=["Unnamed: 0"], inplace=True, errors='ignore')
stock_df.head()

temp_df = pd.read_csv(f"{path}sensor_storage_temperature.csv")
temp_df.drop(columns=["Unnamed: 0"], inplace=True, errors='ignore')
temp_df.head()

"""Now it's up to you, refer back to the steps in your strategic plan to complete this task. Good luck!"""

sales_df.info()

stock_df.info()

temp_df.info()

"""Here, we will look at the data types of each of the column entry

In all the data sets, we will convert the time to datetime datatype.
"""



"""**Data Cleaning**"""

def convert_to_datetime(data: pd.DataFrame = None, column: str = None):

  dummy = data.copy()
  dummy[column] = pd.to_datetime(dummy[column], format='%Y-%m-%d %H:%M:%S')
  return dummy

#let us first convert the timestaml of sales.csv
sales_df = convert_to_datetime(sales_df, 'timestamp')
sales_df.info()

stock_df = convert_to_datetime(stock_df, 'timestamp')
stock_df.info()

temp_df = convert_to_datetime(temp_df, 'timestamp')
temp_df.info()

"""## ***Section : Merge Data***"""



"""Problem statement:
-------
“Can we accurately predict the stock levels of products, based on sales data and sensor data,
on an hourly basis in order to more intelligently procure products from our suppliers.”
"""

from datetime import datetime
def convert_to_hourly(data: pd.DataFrame =  None , column: str  =None):
  dummy = data.copy()
  new_ts = dummy[column].tolist()
  new_ts = [i.strftime('%Y-%m-%d %H:00:00') for i in new_ts] #strftime() method returns a string representing date and time
  new_ts = [datetime.strptime(i,'%Y-%m-%d %H:00:00') for i in new_ts] #The strptime() method creates a datetime object from the given string.
  dummy[column] = new_ts
  return dummy

sales_df = convert_to_hourly(sales_df,'timestamp' )
sales_df.head()

stock_df = convert_to_hourly(stock_df,'timestamp' )
stock_df.head()

temp_df = convert_to_hourly(temp_df,'timestamp' )
temp_df.head()

"""For the sales data, we will group by product id and timestamp.

"""

sales_agg = sales_df.groupby(['timestamp' , 'product_id']).agg({'quantity':'sum'}).reset_index()
sales_agg.head()

"""We will aggregate the data using product id and timestamp in case of the stock.csv and then estimate the stock price by taking mean of the unit price."""

stock_agg = stock_df.groupby(['timestamp' , 'product_id']).agg({'estimated_stock_pct':'mean'}).reset_index()
stock_agg.head()

temp_agg = temp_df.groupby(['timestamp']).agg({'temperature': 'mean'}).reset_index()
temp_agg.head()

"""let us use sales_agg as the base table and merge other tables with it as required.

"""

merged_df = stock_agg.merge(sales_agg, on = ['timestamp' , 'product_id'] , how = 'left' )
merged_df.head()

merged_df = merged_df.merge(temp_agg, on = 'timestamp',how='left')
merged_df.head()

"""Note:
-    inner(default) = intersection
-    outer = union
-    left = intersection + left df
-    right = intersection + right df

"""

merged_df.info()

"""Now, from the above information, we see that the column having the null values is **quantity**. So, we will replace the null values with 0 since this mean there were no sales at that time."""

merged_df['quantity'] = merged_df['quantity'].fillna(0)
merged_df.info()

"""Let us add few more features.

"""

product_cat = sales_df[['product_id','category']]
product_cat = product_cat.drop_duplicates()
product_price = sales_df[['product_id','unit_price']]
product_price = product_price.drop_duplicates()

merged_df = merged_df.merge(product_cat, on="product_id", how="left")
merged_df.head()

merged_df = merged_df.merge(product_price, on="product_id", how="left")
merged_df.head()

merged_df.info()

"""# Handling categorical and numerical values

- Let us first break-down the timestamp into various columns of day, month and hour.
- We will also use dummy variables
 in case of category
"""

merged_df['month'] = merged_df['timestamp'].dt.day
merged_df['week'] = merged_df['timestamp'].dt.dayofweek
merged_df['hour'] = merged_df['timestamp'].dt.hour
merged_df.drop(columns=['timestamp'],inplace=True)
merged_df.head()

merged_df = pd.get_dummies(merged_df , columns = ['category'])
merged_df.head()

merged_df.info()

"""Now only non-numeric values is product ID and it of no use in prediction, so let is remove it."""

merged_df.drop(columns = ['product_id'],inplace = True)
merged_df.info()

"""# Train and Split the model

Let us set aside the target variable and then split the dataset.
Our target variable is **estimated_stock_pct**.
"""

X = merged_df.drop(columns = ['estimated_stock_pct'])
y = merged_df['estimated_stock_pct']
#Let us check the shapes of each set
print("Shape of X is ", X.shape)
print("Shape of y is ", y.shape)

"""Now, let us split the data set and keep 25% for testing. Also, the value of K in K fold = 10

"""

split = 0.75
K = 10

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import StandardScaler

"""Now let us train the K models."""

accuracy = []
for iter in range(0,K):
  model = RandomForestRegressor()
  scaler = StandardScaler()
  X_train, X_test, y_train, y_test  = train_test_split(X,y,train_size = split,random_state = 42)
  scaler.fit(X_train)
  X_train = scaler.transform(X_train)
  X_test = scaler.transform(X_test)
  trained_model = model.fit(X_train,y_train)
  y_pred = trained_model.predict(X_test)
  #Let us check the accuracy of the model using MAE
  mae = mean_absolute_error(y_true = y_test , y_pred=y_pred)
  accuracy.append(mae)
  print(f"Fold {iter + 1}: MAE = {mae:.3f}")
print(f"Average MAE: {(sum(accuracy) / len(accuracy)):.3f}")

print(X_test.shape)
print(y_test.shape)
print(X_train.shape)
print(y_train.shape)

"""Now, let us see which features were important in the prediction."""

import matplotlib.pyplot as plt
import numpy as np
#let us make the list of the features
features = [i.split("__")[0] for i in X.columns] # or features = [i.split(",")[0] for i in X.columns]
importances = model.feature_importances_
indices = np.argsort(importances)
plt.figure(figsize =( 10,20))
plt.title("Feature Importances")
plt.barh(range(len(indices)), importances[indices],color='g')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel("replative importance")
plt.legend()
plt.show()